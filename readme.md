1. Download docker desktop
2. click the Images tab, and on top, search for "ollama"
3. I chose ollama/ollama
4. click the pull button
5. run it, and it will show on the container tab. 
6. ON the container tab, click the ellipsis, and choose open in terminal.
7. in the terminal, type ollama pull llama3, and it will pull the language model.
8. Download "jq", a json parser, and "curl", an api tool, for the enclosed acc.cmd
9. Enclosed 'acc.cmd' will access the api endpoint. 

Invoke the command like so: acc.cmd "What is a LLM"
The result of your query will display back.   
Example shown here.  Sure, it is a lot slower than Claude, 
but it DOES give you good answers on very diverse subjects.

c:\mypath>acc.cmd "What is a LLM"

 What is a LLM

LLM stands for Large Language Model. It's a type of artificial intelligence (AI) model that is trained on a massive amount of text data to learn patterns, relationships, and structures within language.

A Large Language Model is designed to process and generate human-like language, often with impressive capabilities such as:

1. **Text generation**: LLMs can create original text based on a prompt or topic, even generating entire articles, stories, or dialogues.
2. **Language understanding**: They can comprehend complex sentences, nuances of language, and even common sense, allowing them to answer questions, summarize texts, or provide explanations.
3. **Translation**: LLMs can translate text from one language to another, often with high accuracy.
4. **Summarization**: They can condense long pieces of text into shorter summaries while preserving key information.

To achieve these capabilities, Large Language Models are trained on massive datasets, which can include:

1. Web pages
2. Books
3. Articles
4. Social media posts
5. Product reviews

The training process typically involves the following steps:

1. **Data collection**: Gathering a vast amount of text data from various sources.
2. **Tokenization**: Breaking down the text into smaller units, such as words or characters.
3. **Model architecture**: Designing the model's internal structure, including the number of layers, neurons, and connections.
4. **Training**: Feeding the preprocessed data into the model, adjusting parameters to minimize errors and optimize performance.
5. **Fine-tuning**: Adapting the model for specific tasks or domains by tweaking its architecture or training it on additional data.

Some notable examples of Large Language Models include:

1. BERT (Bidirectional Encoder Representations from Transformers)
2. RoBERTa (Robustly Optimized BERT Pretraining Approach)
3. transformer-based models like XLNet and T5

These LLMs have many applications, such as:

1. Natural language processing (NLP) tasks
2. Language translation and localization
3. Content generation (e.g., chatbots, article writing)
4. Question answering and dialogue systems
5. Sentiment analysis and opinion mining

c:\mypath>


